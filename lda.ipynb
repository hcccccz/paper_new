{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "import ipadic\n",
    "from tqdm.notebook import tqdm\n",
    "from argparse import Namespace\n",
    "import os\n",
    "import gensim.corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(data_csv=\"/home/hc/[NII-IDR] 楽天市場データ/sample/sample_from_raw.csv\",\n",
    "                 stopword_dir=\"/home/hc/paper_new//stopword\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把评论论内容分词后写入wakati_review.txt\n",
    "# text = data_sub['レビュー内容'].to_numpy()\n",
    "# with open(\"wakati_review.txt\", \"w\") as fp:\n",
    "#     for line in tqdm(text):\n",
    "#         fp.write(wakati.parse(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hinshi(text, hinshi:str):\n",
    "    \"\"\"\n",
    "    品词, 抽出所有文本对应的品词 text 为pandas review dataframe to numpy。 返回text中所以对应的hinshi\n",
    "    \"\"\"\n",
    "    kigo = set()\n",
    "    CHASEN_ARGS = r' -F \"%m\\t%f[7]\\t%f[6]\\t%F-[0,1,2,3]\\t%f[4]\\t%f[5]\\n\"'\n",
    "    CHASEN_ARGS += r' -U \"%m\\t%m\\t%m\\t%F-[0,1,2,3]\\t\\t\\n\"'\n",
    "    wakati = MeCab.Tagger(ipadic.MECAB_ARGS + CHASEN_ARGS)\n",
    "    for line in text:\n",
    "        wakati_text = wakati.parse(line).split(\"\\n\")\n",
    "        for token in wakati_text:\n",
    "            token = token.split(\"\\t\")\n",
    "            if token[0] != \"\" and token[0] != \"EOS\":\n",
    "                if hinshi in token[3]:\n",
    "                    kigo.add(token[0])\n",
    "    return kigo\n",
    "\n",
    "data_sub = pd.read_csv(\"/home/hc/[NII-IDR] 楽天市場データ/sample/sample_from_raw.csv\")['レビュー内容']\n",
    "text = data_sub.to_numpy()\n",
    "kigo = extract_hinshi(text,\"副詞\")\n",
    "with open(os.path.join(args.stopword_dir,\"副詞.txt\"), \"w\") as fp:\n",
    "    for line in kigo:\n",
    "        fp.write(line)\n",
    "        fp.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write wakati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = {}\n",
    "    def add_token(self, token):\n",
    "        try:\n",
    "            index = self.token_to_idx[token]\n",
    "        except KeyError:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "    def add_many(self, tokens:list): #1d list of token\n",
    "        for token in tokens:\n",
    "            self.add_token(token)\n",
    "    def add_corpus(self, doc:list): #2d list of list\n",
    "        for sent in doc:\n",
    "            self.add_many(sent)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary> size is {}\".format(len(self.token_to_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemma(text:list): #1d list\n",
    "    CHASEN_ARGS = r' -F \"%m\\t%f[7]\\t%f[6]\\t%F-[0,1,2,3]\\t%f[4]\\t%f[5]\\n\"'\n",
    "    CHASEN_ARGS += r' -U \"%m\\t%m\\t%m\\t%F-[0,1,2,3]\\t\\t\\n\"'\n",
    "    wakati = MeCab.Tagger(ipadic.MECAB_ARGS + CHASEN_ARGS)\n",
    "    temp_text = \"\".join(text)\n",
    "    wakati_text = wakati.parse(temp_text).split(\"\\n\")\n",
    "    dic_t = {}\n",
    "    for token in wakati_text:\n",
    "        token = token.split(\"\\t\")\n",
    "        if token[0] != \"\" and token[0] != \"EOS\":\n",
    "            dic_t[token[0]] = token[2]\n",
    "    for idx in range(len(text)):\n",
    "        if text[idx] in dic_t:\n",
    "            text[idx] = dic_t[text[idx]]\n",
    "    return text #list of lemmalization\n",
    "\n",
    "def wakati_tolist(text:list, stopword:list=[]): #list of str\n",
    "    #convert list of str to list of list, removing stopword and \\n\n",
    "\n",
    "    wakati = MeCab.Tagger(\"-Owakati\")\n",
    "    for line_idx in range(len(text)):\n",
    "        line = wakati.parse(text[line_idx])\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        line_list = line.split()\n",
    "        line_list = [token for token in line_list if token not in stopword] #1d list\n",
    "        line_list = lemma(line_list)\n",
    "        text[line_idx] = line_list\n",
    "    return text #list of list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "for stopword_file in os.listdir(\"stopword\"):\n",
    "    with open(os.path.join(\"stopword\",stopword_file),\"r\") as fp:\n",
    "        stopword = [line.replace(\"\\n\",\"\") for line in fp.readlines()]\n",
    "    stopwords += stopword\n",
    "\n",
    "data = pd.read_csv(\"/home/hc/[NII-IDR] 楽天市場データ/sample/sample_from_raw2.csv\")\n",
    "text = data['レビュー内容'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gr1 = data[data['参考になった数']>=1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data_gr1['レビュー内容'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33852\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = wakati_tolist(text,stopword=stopwords) #list of list\n",
    "id2word = gensim.corpora.Dictionary(text)\n",
    "print(len(id2word))\n",
    "doc_term_matrix = [id2word.doc2bow(t) for t in text]\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=doc_term_matrix,id2word=id2word,num_topics=6,iterations=400 )\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.coherencemodel import CoherenceModel\n",
    "# import matplotlib.pyplot as plt\n",
    "# num_t = []\n",
    "# c_score = []\n",
    "# for num_topic in tqdm(range(3,20)):\n",
    "#     lda_model = gensim.models.LdaModel(corpus=doc_term_matrix,id2word=id2word,num_topics=num_topic)\n",
    "#     cm = CoherenceModel(model=lda_model,corpus=doc_term_matrix,texts=text,dictionary=id2word)\n",
    "#     c_score.append(cm.get_coherence())\n",
    "#     num_t.append(num_topic)\n",
    "\n",
    "\n",
    "# plt.plot(num_t,c_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "lda_prepare = gensimvis.prepare(lda_model,doc_term_matrix,id2word)\n",
    "pyLDAvis.save_html(lda_prepare,\"lda1.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.007*\"用\" + 0.007*\"思っ\" + 0.005*\"値段\" + 0.005*\"満足\" + 0.004*\"使い\" + 0.004*\"サイズ\" '\n",
      "  '+ 0.003*\"見\" + 0.003*\"時\" + 0.003*\"ありがとう\" + 0.003*\"だけ\"'),\n",
      " (1,\n",
      "  '0.005*\"使っ\" + 0.005*\"色\" + 0.004*\"日\" + 0.004*\"けど\" + 0.004*\"良かっ\" + 0.004*\"使い\" '\n",
      "  '+ 0.004*\"目\" + 0.004*\"感\" + 0.004*\"入っ\" + 0.003*\"もう\"'),\n",
      " (2,\n",
      "  '0.006*\"サイズ\" + 0.005*\"満足\" + 0.005*\"み\" + 0.004*\"色\" + 0.004*\"しっかり\" + '\n",
      "  '0.004*\"使い\" + 0.004*\"生地\" + 0.004*\"見\" + 0.004*\"着\" + 0.003*\"入っ\"'),\n",
      " (3,\n",
      "  '0.005*\"良かっ\" + 0.005*\"だけ\" + 0.005*\"入っ\" + 0.005*\"注文\" + 0.004*\"使い\" + '\n",
      "  '0.004*\"サイズ\" + 0.004*\"時\" + 0.004*\"思っ\" + 0.004*\"すぐ\" + 0.004*\"み\"'),\n",
      " (4,\n",
      "  '0.007*\"色\" + 0.006*\"サイズ\" + 0.004*\"注文\" + 0.004*\"時\" + 0.004*\"着\" + 0.004*\"思っ\" + '\n",
      "  '0.004*\"使っ\" + 0.004*\"値段\" + 0.004*\"さん\" + 0.004*\"日\"'),\n",
      " (5,\n",
      "  '0.012*\"サイズ\" + 0.004*\"用\" + 0.004*\"見\" + 0.004*\"日\" + 0.003*\"満足\" + 0.003*\"買っ\" + '\n",
      "  '0.003*\"思っ\" + 0.003*\"感\" + 0.003*\"み\" + 0.003*\"時\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stopword 44412\n",
    "+puncts 44351\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
