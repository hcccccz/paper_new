{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "import ipadic\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = \"\"\n",
    "unzip_path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Wakati reviview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "data_sub = pd.read_csv(sample_path)['レビュー内容']\n",
    "text = data_sub.to_numpy()\n",
    "with open(\"wakati_review.txt\", \"w\") as fp:\n",
    "    for line in tqdm(text):\n",
    "        fp.write(wakati.parse(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly print a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHASEN_ARGS = r' -F \"%m\\t%f[7]\\t%f[6]\\t%F-[0,1,2,3]\\t%f[4]\\t%f[5]\\n\"'\n",
    "CHASEN_ARGS += r' -U \"%m\\t%m\\t%m\\t%F-[0,1,2,3]\\t\\t\\n\"'\n",
    "\n",
    "wakati = MeCab.Tagger(ipadic.MECAB_ARGS + CHASEN_ARGS)\n",
    "\n",
    "\n",
    "data = pd.read_csv(sample_path)\n",
    "data_sub = data['レビュー内容'].to_numpy()\n",
    "select = np.random.choice(data_sub)\n",
    "results = wakati.parse(select)\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "\n",
    "    def __init__(self,path, sample_size):\n",
    "        self.path = path #path to raw\n",
    "        self.data_saved = os.path.join(\"/home/hc/[NII-IDR] 楽天市場データ/review/\",\"sample\")\n",
    "        if not os.path.isdir(self.data_saved):\n",
    "            os.mkdir(self.data_saved)\n",
    "        self.encoded_out_dir = os.path.join(self.data_saved,\"file_encoded.txt\")\n",
    "        self.sample_index_outdir = os.path.join(self.data_saved,\"file_sample.txt\")\n",
    "        self.sample_csv_out_dir = os.path.join(self.data_saved,\"sample_from_raw.csv\")\n",
    "        self.sample_size = sample_size\n",
    "def extract_hinshi(text, hinshi:str):\n",
    "    \"\"\"\n",
    "    品词, 抽出所有文本对应的品词 text 为pandas review dataframe to numpy。 返回text中所以对应的hinshi\n",
    "    \"\"\"\n",
    "    wakati = MeCab.Tagger(ipadic.MECAB_ARGS + CHASEN_ARGS)\n",
    "    kigo = set()\n",
    "    CHASEN_ARGS = r' -F \"%m\\t%f[7]\\t%f[6]\\t%F-[0,1,2,3]\\t%f[4]\\t%f[5]\\n\"'\n",
    "    CHASEN_ARGS += r' -U \"%m\\t%m\\t%m\\t%F-[0,1,2,3]\\t\\t\\n\"'\n",
    "\n",
    "    for line in text:\n",
    "        for token in wakati_text:\n",
    "            token = token.split(\"\\t\")\n",
    "            if token[0] != \"\" and token[0] != \"EOS\":\n",
    "                if hinshi in token[3]:\n",
    "                    kigo.add(token[0])\n",
    "    return kigo\n",
    "\n",
    "\n",
    "\n",
    "    def file_encode(self):\n",
    "        \"\"\"\n",
    "        读取所有的评论,将index和参考になった数存下来保存到out_name文件下\"\"\"\n",
    "\n",
    "        self.file_list = sorted(os.listdir(self.path)) #列出所以文件\n",
    "        with open(self.encoded_out_dir, \"w\") as file: #file_encoded.txt\n",
    "            for file_name in tqdm(self.file_list):\n",
    "                data = pd.read_csv(os.path.join(self.path,file_name),sep='\\t',quoting=csv.QUOTE_NONE)\n",
    "                col = ['投稿者ID', '店舗名', '店舗ID', '商品名', '商品ID', '商品ページURL', '商品ジャンルID', '商品ジャンルIDパス', '使い道',\n",
    "                        '目的', '頻度', '評価ポイント', 'レビュータイトル', 'レビュー内容', '参考になった数', 'レビュー登録日時']\n",
    "                data.columns = col\n",
    "                data.reset_index(inplace=True) #add index column\n",
    "                data = data[[\"index\",\"参考になった数\"]].to_numpy()\n",
    "                for line in range(data.shape[0]):\n",
    "                    content = file_name + \" \" + \" \".join(str(i) for i in data[line,:].tolist())\n",
    "                    file.write(content + \"\\n\")\n",
    "\n",
    "    def sample_file(self):\n",
    "        path_to_encoded = self.encoded_out_dir\n",
    "        path_to_sample = self.sample_index_outdir\n",
    "        if not os.path.exists(path_to_encoded):\n",
    "            self.file_encode()\n",
    "        else:\n",
    "            print(\"encoded exist\")\n",
    "            with open(path_to_encoded,\"r\") as file:\n",
    "                encoded = file.readlines()\n",
    "\n",
    "            sample = np.random.choice(range(len(encoded)),size=self.sample_size, replace=False)\n",
    "            encoded_sample = [encoded[i] for i in sample] #slicing wit list of index\n",
    "        with open(path_to_sample,\"w\") as file:\n",
    "            for line in encoded_sample:\n",
    "                file.write(line)\n",
    "\n",
    "    def entry_from_row(self):\n",
    "        path_to_sample = self.sample_index_outdir\n",
    "        if not os.path.exists(path_to_sample):\n",
    "            self.sample_file()\n",
    "        else:\n",
    "            print(\"sample file exist\")\n",
    "            with open(path_to_sample, \"r\") as file:\n",
    "                sample = {}\n",
    "                for line in file:\n",
    "                    line = line.replace(\"\\n\", \"\").split(\" \")\n",
    "                    line_index = sample.setdefault(line[0],[]) #key is file name, value is list of index\n",
    "                    line_index.append(line[1])\n",
    "            frames = []\n",
    "            print(\"start to sampling\")\n",
    "            for key in tqdm(sample.keys()):\n",
    "                path = os.path.join(self.path,key)\n",
    "                df = pd.read_csv(path,sep='\\t',quoting=csv.QUOTE_NONE)\n",
    "                col = ['投稿者ID', '店舗名', '店舗ID', '商品名', '商品ID', '商品ページURL', '商品ジャンルID', '商品ジャンルIDパス', '使い道',\n",
    "                        '目的', '頻度', '評価ポイント', 'レビュータイトル', 'レビュー内容', '参考になった数', 'レビュー登録日時']\n",
    "                df.columns = col\n",
    "                df.reset_index(inplace=True)\n",
    "                idxs = [int(idx) for idx in sample[key]]\n",
    "                frame = df.iloc[idxs,:].copy()\n",
    "                frames.append(frame)\n",
    "            result = pd.concat(frames)\n",
    "            result.reset_index(inplace=True)\n",
    "            result.to_csv(self.sample_csv_out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Hinshi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hinshi(text, hinshi:str):\n",
    "    \"\"\"\n",
    "    品词, 抽出所有文本对应的品词 text 为pandas review dataframe to numpy。 返回text中所以对应的hinshi\n",
    "    \"\"\"\n",
    "    wakati = MeCab.Tagger(ipadic.MECAB_ARGS + CHASEN_ARGS)\n",
    "    kigo = set()\n",
    "    CHASEN_ARGS = r' -F \"%m\\t%f[7]\\t%f[6]\\t%F-[0,1,2,3]\\t%f[4]\\t%f[5]\\n\"'\n",
    "    CHASEN_ARGS += r' -U \"%m\\t%m\\t%m\\t%F-[0,1,2,3]\\t\\t\\n\"'\n",
    "\n",
    "    for line in text:\n",
    "        for token in wakati_text:\n",
    "            token = token.split(\"\\t\")\n",
    "            if token[0] != \"\" and token[0] != \"EOS\":\n",
    "                if hinshi in token[3]:\n",
    "                    kigo.add(token[0])\n",
    "    return kigo\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
