{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvA1Sx7scN96",
        "outputId": "72aeacea-f35c-480d-fe3e-57934ec8e4b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: unidic-lite in /usr/local/lib/python3.10/dist-packages (1.0.8)\n",
            "Requirement already satisfied: ipadic in /usr/local/lib/python3.10/dist-packages (1.0.0)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmdzQjnzb6R2",
        "outputId": "85d23a1b-3305-4100-b6a0-5334a21aa314"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import MeCab\n",
        "import ipadic\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "UartB430cnpq"
      },
      "outputs": [],
      "source": [
        "# os.listdir(\"/content/drive/MyDrive/data/\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KsIx9jR5b6R2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPz3u0oTb6R2"
      },
      "source": [
        "### Generate Wakati reviview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KKlKgUDb6R3"
      },
      "outputs": [],
      "source": [
        "wakati = MeCab.Tagger(\"-Owakati\")\n",
        "data_sub = pd.read_csv(sample_path)['レビュー内容']\n",
        "text = data_sub.to_numpy()\n",
        "with open(\"wakati_review.txt\", \"w\") as fp:\n",
        "    for line in tqdm(text):\n",
        "        fp.write(wakati.parse(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v75FgsZrb6R3"
      },
      "source": [
        "### Randomly print a review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Y6OWyS07b6R3",
        "outputId": "ea85d8db-affa-4438-ca5f-dd630de500a4"
      },
      "outputs": [
        {
          "ename": "IsADirectoryError",
          "evalue": "[Errno 21] Is a directory: '/content/drive/MyDrive/data/sample'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-448b4bcfb8ea>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdata_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'レビュー内容'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mselect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/drive/MyDrive/data/sample'"
          ]
        }
      ],
      "source": [
        "CHASEN_ARGS = r' -F \"%m\\t%f[7]\\t%f[6]\\t%F-[0,1,2,3]\\t%f[4]\\t%f[5]\\n\"'\n",
        "CHASEN_ARGS += r' -U \"%m\\t%m\\t%m\\t%F-[0,1,2,3]\\t\\t\\n\"'\n",
        "\n",
        "wakati = MeCab.Tagger(ipadic.MECAB_ARGS + CHASEN_ARGS)\n",
        "\n",
        "\n",
        "data = pd.read_csv(sample_path)\n",
        "data_sub = data['レビュー内容'].to_numpy()\n",
        "select = np.random.choice(data_sub)\n",
        "results = wakati.parse(select)\n",
        "print(results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4LOaO4Sb6R3"
      },
      "source": [
        "### Sampler class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CpsCoeDBb6R3"
      },
      "outputs": [],
      "source": [
        "def balance_sample(data:np.ndarray, size:int, cut_off):\n",
        "    data = np.where(data > cut_off, 1, 0) #\n",
        "    data_frame = np.array([np.arange(data.shape[0]),data]).T #first column index, second column 1 or 0\n",
        "    pos = (data_frame[:,1] == 1) #second column equal to 1\n",
        "    neg = (data_frame[:,1] == 0) #second column equal to 0\n",
        "    neg_sample = np.random.choice(data_frame[neg,0],size = round(size/2), replace = False) #select index that is neg\n",
        "    pos_sample = np.random.choice(data_frame[pos,0],size = round(size/2), replace = False) #select index that is pos\n",
        "    return neg_sample,pos_sample\n",
        "\n",
        "\n",
        "\n",
        "class Sampler:\n",
        "\n",
        "    def __init__(self,path,data_saved, sample_size):\n",
        "        self.path = path #path to raw\n",
        "        self.data_saved = data_saved\n",
        "        if not os.path.isdir(self.data_saved):\n",
        "            os.mkdir(self.data_saved)\n",
        "        self.encoded_out_dir = os.path.join(self.data_saved,\"file_encoded.txt\")\n",
        "        self.sample_index_outdir = os.path.join(self.data_saved,\"file_sample.txt\")\n",
        "        self.sample_csv_out_dir = os.path.join(self.data_saved,\"sample_from_raw.csv\")\n",
        "        self.sample_size = sample_size\n",
        "\n",
        "    def file_encode(self):\n",
        "        \"\"\"\n",
        "        读取所有的评论,将index和参考になった数存下来保存到out_name文件下\"\"\"\n",
        "\n",
        "        self.file_list = sorted(os.listdir(self.path)) #列出所以文件\n",
        "        with open(self.encoded_out_dir, \"w\") as file: #file_encoded.txt\n",
        "            for file_name in tqdm(self.file_list):\n",
        "                data = pd.read_csv(os.path.join(self.path,file_name),sep='\\t',quoting=csv.QUOTE_NONE)\n",
        "                col = ['投稿者ID', '店舗名', '店舗ID', '商品名', '商品ID', '商品ページURL', '商品ジャンルID', '商品ジャンルIDパス', '使い道',\n",
        "                        '目的', '頻度', '評価ポイント', 'レビュータイトル', 'レビュー内容', '参考になった数', 'レビュー登録日時']\n",
        "                data.columns = col\n",
        "                data.reset_index(inplace=True) #add index column\n",
        "                data = data[[\"index\",\"参考になった数\"]].to_numpy()\n",
        "                for line in range(data.shape[0]):\n",
        "                    content = file_name + \" \" + \" \".join(str(i) for i in data[line,:].tolist())\n",
        "                    file.write(content + \"\\n\")\n",
        "\n",
        "    def sample_file(self, cut_off,balance=False):\n",
        "        path_to_encoded = self.encoded_out_dir\n",
        "        path_to_sample = self.sample_index_outdir\n",
        "\n",
        "        if not os.path.exists(path_to_encoded):\n",
        "            self.file_encode()\n",
        "        else:\n",
        "            print(\"encoded exist\")\n",
        "\n",
        "            if balance:\n",
        "                with open(path_to_encoded,\"r\") as fp:\n",
        "                    target = []\n",
        "                    for i in fp:\n",
        "                        target.append(int(i.split()[2])) #append the number of usefulness\n",
        "                    target = np.array(target)\n",
        "                    neg, pos = balance_sample(data=target, size=50000, cut_off=cut_off) #index of neg and pos\n",
        "                    samp = np.append(neg, pos)\n",
        "                with open(path_to_encoded, \"r\") as fp:\n",
        "                    data = fp.readlines()\n",
        "                encoded_sample = [data[i] for i in samp]\n",
        "            else:\n",
        "                with open(path_to_encoded,\"r\") as file:\n",
        "                    encoded = file.readlines()\n",
        "\n",
        "                sample = np.random.choice(range(len(encoded)),size=self.sample_size, replace=False) #sample from index\n",
        "                encoded_sample = [encoded[i] for i in sample] #slicing wit list of index\n",
        "            with open(path_to_sample,\"w\") as file:\n",
        "                for line in encoded_sample:\n",
        "                    file.write(line)\n",
        "\n",
        "    def entry_from_row(self,cut_off,balance=False):\n",
        "        path_to_sample = self.sample_index_outdir\n",
        "        if not os.path.exists(path_to_sample):\n",
        "            self.sample_file(cut_off,balance=balance)\n",
        "        else:\n",
        "            print(\"sample file exist\")\n",
        "            with open(path_to_sample, \"r\") as file:\n",
        "                sample = {}\n",
        "                for line in file:\n",
        "                    line = line.replace(\"\\n\", \"\").split(\" \")\n",
        "                    line_index = sample.setdefault(line[0],[]) #key is file name, value is list of index\n",
        "                    line_index.append(line[1])\n",
        "            frames = []\n",
        "            print(\"start to sampling\")\n",
        "            for key in tqdm(sample.keys()):\n",
        "                path = os.path.join(self.path,key)\n",
        "                df = pd.read_csv(path,sep='\\t',quoting=csv.QUOTE_NONE)\n",
        "                col = ['投稿者ID', '店舗名', '店舗ID', '商品名', '商品ID', '商品ページURL', '商品ジャンルID', '商品ジャンルIDパス', '使い道',\n",
        "                        '目的', '頻度', '評価ポイント', 'レビュータイトル', 'レビュー内容', '参考になった数', 'レビュー登録日時']\n",
        "                df.columns = col\n",
        "                df.reset_index(inplace=True)\n",
        "                idxs = [int(idx) for idx in sample[key]]\n",
        "                frame = df.iloc[idxs,:].copy()\n",
        "                frames.append(frame)\n",
        "            result = pd.concat(frames)\n",
        "            result.reset_index(inplace=True)\n",
        "            result.to_csv(self.sample_csv_out_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf1ckkedb6R4"
      },
      "source": [
        "### Extract Hinshi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QtJE3R31b6R4"
      },
      "outputs": [],
      "source": [
        "def extract_hinshi(text, hinshi:str):\n",
        "    \"\"\"\n",
        "    品词, 抽出所有文本对应的品词 text 为pandas review dataframe to numpy。 返回text中所以对应的hinshi\n",
        "    \"\"\"\n",
        "    wakati = MeCab.Tagger(ipadic.MECAB_ARGS + CHASEN_ARGS)\n",
        "    kigo = set()\n",
        "    CHASEN_ARGS = r' -F \"%m\\t%f[7]\\t%f[6]\\t%F-[0,1,2,3]\\t%f[4]\\t%f[5]\\n\"'\n",
        "    CHASEN_ARGS += r' -U \"%m\\t%m\\t%m\\t%F-[0,1,2,3]\\t\\t\\n\"'\n",
        "\n",
        "    for line in text:\n",
        "        for token in wakati_text:\n",
        "            token = token.split(\"\\t\")\n",
        "            if token[0] != \"\" and token[0] != \"EOS\":\n",
        "                if hinshi in token[3]:\n",
        "                    kigo.add(token[0])\n",
        "    return kigo\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozXVibe2d0Po"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukSEd0PbcCnZ",
        "outputId": "affd6255-24a0-4189-995c-9e51eea9e155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample file exist\n",
            "start to sampling\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/60 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 60/60 [09:11<00:00,  9.20s/it]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import csv\n",
        "sample_path = \"/home/hc/[NII-IDR] 楽天市場データ/review/sample\"\n",
        "unzip_path = \"/home/hc/[NII-IDR] 楽天市場データ/review/unzip\"\n",
        "sampler = Sampler(path=unzip_path, data_saved= sample_path, sample_size=50000)\n",
        "sampler.entry_from_row(balance=True,cut_off=5) #4以及4以上的为一组"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tensor",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
