{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mecab-python3\n",
        "!pip install unidic-lite\n",
        "!pip install ipadic"
      ],
      "metadata": {
        "id": "SvA1Sx7scN96",
        "outputId": "713a39eb-2009-41e0-bb14-29a924ecb2c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
            "Downloading mecab_python3-1.0.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-1.0.9\n",
            "Collecting unidic-lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658818 sha256=6b9e9014c600bf828a0c09dcff1c0da66e91aebcbee51f86802a425a48d106f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite\n",
            "Successfully installed unidic-lite-1.0.8\n",
            "Collecting ipadic\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ipadic\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556704 sha256=4b80b00d4c0b3dece57c42b06177b36c86d2d404e2f7a27072b9c05592bc0d7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ea/e3/2f6e0860a327daba3b030853fce4483ed37468bbf1101c59c3\n",
            "Successfully built ipadic\n",
            "Installing collected packages: ipadic\n",
            "Successfully installed ipadic-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PmdzQjnzb6R2",
        "outputId": "880571bf-923d-46d6-cd7d-2d231ac52779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import MeCab\n",
        "import ipadic\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BcYRGkzTcwmN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"/content/drive/MyDrive/data/\")"
      ],
      "metadata": {
        "id": "UartB430cnpq",
        "outputId": "8e796388-1980-472f-bf8d-645452be792f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['unzip', 'sample']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KsIx9jR5b6R2"
      },
      "outputs": [],
      "source": [
        "sample_path = \"/content/drive/MyDrive/data/sample\"\n",
        "unzip_path = \"/content/drive/MyDrive/data/unzip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPz3u0oTb6R2"
      },
      "source": [
        "### Generate Wakati reviview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KKlKgUDb6R3"
      },
      "outputs": [],
      "source": [
        "wakati = MeCab.Tagger(\"-Owakati\")\n",
        "data_sub = pd.read_csv(sample_path)['レビュー内容']\n",
        "text = data_sub.to_numpy()\n",
        "with open(\"wakati_review.txt\", \"w\") as fp:\n",
        "    for line in tqdm(text):\n",
        "        fp.write(wakati.parse(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v75FgsZrb6R3"
      },
      "source": [
        "### Randomly print a review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6OWyS07b6R3"
      },
      "outputs": [],
      "source": [
        "CHASEN_ARGS = r' -F \"%m\\t%f[7]\\t%f[6]\\t%F-[0,1,2,3]\\t%f[4]\\t%f[5]\\n\"'\n",
        "CHASEN_ARGS += r' -U \"%m\\t%m\\t%m\\t%F-[0,1,2,3]\\t\\t\\n\"'\n",
        "\n",
        "wakati = MeCab.Tagger(ipadic.MECAB_ARGS + CHASEN_ARGS)\n",
        "\n",
        "\n",
        "data = pd.read_csv(sample_path)\n",
        "data_sub = data['レビュー内容'].to_numpy()\n",
        "select = np.random.choice(data_sub)\n",
        "results = wakati.parse(select)\n",
        "print(results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4LOaO4Sb6R3"
      },
      "source": [
        "### Sampler class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpsCoeDBb6R3"
      },
      "outputs": [],
      "source": [
        "class Sampler:\n",
        "\n",
        "    def __init__(self,path, sample_size):\n",
        "        self.path = path #path to raw\n",
        "        self.data_saved = os.path.join(\"/home/hc/[NII-IDR] 楽天市場データ/review/\",\"sample\")\n",
        "        if not os.path.isdir(self.data_saved):\n",
        "            os.mkdir(self.data_saved)\n",
        "        self.encoded_out_dir = os.path.join(self.data_saved,\"file_encoded.txt\")\n",
        "        self.sample_index_outdir = os.path.join(self.data_saved,\"file_sample.txt\")\n",
        "        self.sample_csv_out_dir = os.path.join(self.data_saved,\"sample_from_raw.csv\")\n",
        "        self.sample_size = sample_size\n",
        "def extract_hinshi(text, hinshi:str):\n",
        "    \"\"\"\n",
        "    品词, 抽出所有文本对应的品词 text 为pandas review dataframe to numpy。 返回text中所以对应的hinshi\n",
        "    \"\"\"\n",
        "    wakati = MeCab.Tagger(ipadic.MECAB_ARGS + CHASEN_ARGS)\n",
        "    kigo = set()\n",
        "    CHASEN_ARGS = r' -F \"%m\\t%f[7]\\t%f[6]\\t%F-[0,1,2,3]\\t%f[4]\\t%f[5]\\n\"'\n",
        "    CHASEN_ARGS += r' -U \"%m\\t%m\\t%m\\t%F-[0,1,2,3]\\t\\t\\n\"'\n",
        "\n",
        "    for line in text:\n",
        "        for token in wakati_text:\n",
        "            token = token.split(\"\\t\")\n",
        "            if token[0] != \"\" and token[0] != \"EOS\":\n",
        "                if hinshi in token[3]:\n",
        "                    kigo.add(token[0])\n",
        "    return kigo\n",
        "\n",
        "\n",
        "\n",
        "    def file_encode(self):\n",
        "        \"\"\"\n",
        "        读取所有的评论,将index和参考になった数存下来保存到out_name文件下\"\"\"\n",
        "\n",
        "        self.file_list = sorted(os.listdir(self.path)) #列出所以文件\n",
        "        with open(self.encoded_out_dir, \"w\") as file: #file_encoded.txt\n",
        "            for file_name in tqdm(self.file_list):\n",
        "                data = pd.read_csv(os.path.join(self.path,file_name),sep='\\t',quoting=csv.QUOTE_NONE)\n",
        "                col = ['投稿者ID', '店舗名', '店舗ID', '商品名', '商品ID', '商品ページURL', '商品ジャンルID', '商品ジャンルIDパス', '使い道',\n",
        "                        '目的', '頻度', '評価ポイント', 'レビュータイトル', 'レビュー内容', '参考になった数', 'レビュー登録日時']\n",
        "                data.columns = col\n",
        "                data.reset_index(inplace=True) #add index column\n",
        "                data = data[[\"index\",\"参考になった数\"]].to_numpy()\n",
        "                for line in range(data.shape[0]):\n",
        "                    content = file_name + \" \" + \" \".join(str(i) for i in data[line,:].tolist())\n",
        "                    file.write(content + \"\\n\")\n",
        "\n",
        "    def sample_file(self):\n",
        "        path_to_encoded = self.encoded_out_dir\n",
        "        path_to_sample = self.sample_index_outdir\n",
        "        if not os.path.exists(path_to_encoded):\n",
        "            self.file_encode()\n",
        "        else:\n",
        "            print(\"encoded exist\")\n",
        "            with open(path_to_encoded,\"r\") as file:\n",
        "                encoded = file.readlines()\n",
        "\n",
        "            sample = np.random.choice(range(len(encoded)),size=self.sample_size, replace=False)\n",
        "            encoded_sample = [encoded[i] for i in sample] #slicing wit list of index\n",
        "        with open(path_to_sample,\"w\") as file:\n",
        "            for line in encoded_sample:\n",
        "                file.write(line)\n",
        "\n",
        "    def entry_from_row(self):\n",
        "        path_to_sample = self.sample_index_outdir\n",
        "        if not os.path.exists(path_to_sample):\n",
        "            self.sample_file()\n",
        "        else:\n",
        "            print(\"sample file exist\")\n",
        "            with open(path_to_sample, \"r\") as file:\n",
        "                sample = {}\n",
        "                for line in file:\n",
        "                    line = line.replace(\"\\n\", \"\").split(\" \")\n",
        "                    line_index = sample.setdefault(line[0],[]) #key is file name, value is list of index\n",
        "                    line_index.append(line[1])\n",
        "            frames = []\n",
        "            print(\"start to sampling\")\n",
        "            for key in tqdm(sample.keys()):\n",
        "                path = os.path.join(self.path,key)\n",
        "                df = pd.read_csv(path,sep='\\t',quoting=csv.QUOTE_NONE)\n",
        "                col = ['投稿者ID', '店舗名', '店舗ID', '商品名', '商品ID', '商品ページURL', '商品ジャンルID', '商品ジャンルIDパス', '使い道',\n",
        "                        '目的', '頻度', '評価ポイント', 'レビュータイトル', 'レビュー内容', '参考になった数', 'レビュー登録日時']\n",
        "                df.columns = col\n",
        "                df.reset_index(inplace=True)\n",
        "                idxs = [int(idx) for idx in sample[key]]\n",
        "                frame = df.iloc[idxs,:].copy()\n",
        "                frames.append(frame)\n",
        "            result = pd.concat(frames)\n",
        "            result.reset_index(inplace=True)\n",
        "            result.to_csv(self.sample_csv_out_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf1ckkedb6R4"
      },
      "source": [
        "### Extract Hinshi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtJE3R31b6R4"
      },
      "outputs": [],
      "source": [
        "def extract_hinshi(text, hinshi:str):\n",
        "    \"\"\"\n",
        "    品词, 抽出所有文本对应的品词 text 为pandas review dataframe to numpy。 返回text中所以对应的hinshi\n",
        "    \"\"\"\n",
        "    wakati = MeCab.Tagger(ipadic.MECAB_ARGS + CHASEN_ARGS)\n",
        "    kigo = set()\n",
        "    CHASEN_ARGS = r' -F \"%m\\t%f[7]\\t%f[6]\\t%F-[0,1,2,3]\\t%f[4]\\t%f[5]\\n\"'\n",
        "    CHASEN_ARGS += r' -U \"%m\\t%m\\t%m\\t%F-[0,1,2,3]\\t\\t\\n\"'\n",
        "\n",
        "    for line in text:\n",
        "        for token in wakati_text:\n",
        "            token = token.split(\"\\t\")\n",
        "            if token[0] != \"\" and token[0] != \"EOS\":\n",
        "                if hinshi in token[3]:\n",
        "                    kigo.add(token[0])\n",
        "    return kigo\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampl"
      ],
      "metadata": {
        "id": "ukSEd0PbcCnZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}